---
description: Chain Development Context
globs: 
  - "src/chains/**/*.js"
alwaysApply: false
---
# Chain Development Context

You are working on **chains** - complex multi-step AI workflows in the Verblets library.

## Chain Characteristics

- **Complex workflows** with multiple LLM calls
- **Batch processing** with configurable chunk sizes
- **Retry logic** with `maxAttempts` configuration
- **Model negotiation** for optimal performance/cost balance
- **Always require comprehensive error handling**

## Common Chain Patterns

### Standard Chain Function
```javascript
export default async function chainName(input, instructions, config = {}) {
  const { 
    chunkSize = 10, 
    maxAttempts = 3, 
    llm = { negotiate: { fast: true, cheap: true } },
    ...options 
  } = config;
  
  // Batch processing logic
  for (let i = 0; i < input.length; i += chunkSize) {
    const batch = input.slice(i, i + chunkSize);
    // eslint-disable-next-line no-await-in-loop
    const result = await chatGPT(prompt, {
      modelOptions: { ...llm },
      ...options
    });
  }
}
```

### Model Selection for Chains
- **Default**: `{ negotiate: { fast: true, cheap: true } }` for bulk operations
- **Quality**: `{ negotiate: { good: true } }` for critical analysis
- **Privacy**: `{ modelName: 'privacy' }` for sensitive data
- **Reasoning**: `{ negotiate: { reasoning: true } }` for complex logic

### Required Files
- `index.js` - Main implementation
- `index.examples.js` - Real API usage examples with `aiExpect`
- `index.spec.js` - Unit tests with mocked LLM calls
- `README.md` - Documentation following guidelines
- `schema.json` or `*-result.json` - JSON schemas for structured outputs

## Chain-Specific Guidelines

### Batch Processing
- Use configurable `chunkSize` with sensible defaults
- Implement progress tracking for long operations
<<<<<<< HEAD

### Error Handling
- Retry logic with exponential backoff
- Provide meaningful error messages
- Use retry to handle LLM call failures 

### Performance Optimization
- Use appropriate concurrency limits
=======
- Handle partial failures gracefully
- Consider memory efficiency for large datasets

### Error Handling
- Retry logic with exponential backoff
- Validate intermediate results
- Provide meaningful error messages
- Handle LLM call failures gracefully

### Performance Optimization
- Configure bulk sizes based on complexity
- Use appropriate concurrency limits
- Monitor and adjust model selection
- Implement inactivity timeouts for reliability
>>>>>>> 2011260 (add document reducer implementation)

Reference `guidelines/CODE_QUALITY.md` for error handling requirements and `guidelines/ARCHITECTURE_TESTS.md` for testing patterns.