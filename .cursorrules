# Verblets AI Library - Cursor Rules

You are assisting with the Verblets AI library - a collection of tools for building LLM-powered applications using Node.js.

## ⚠️ NO COMMENTS RULE

**NEVER add comments when making code changes.** Only add comments when:
1. User explicitly requests comments
2. Complex algorithms require explanation
3. Non-obvious edge cases exist

## Project Guidelines

**Architecture & Standards**: Follow guidelines in `guidelines/CODE_QUALITY.md`, `guidelines/DOCUMENTATION.md`, and `guidelines/ARCHITECTURE_TESTS.md`

## File Organization

```
src/
├── chains/          # Multi-step AI workflows, model negotiation for optimization (kebab-case directories)
├── verblets/        # Simple LLM transformations, usually fastGoodCheap default
├── lib/             # Utilities (chatgpt, model-service, each-file, etc.)
├── prompts/         # Prompt generation functions
├── services/        # Core services (llm-model negotiation)
└── json-schemas/    # Validation schemas for structured outputs

guidelines/          # Architecture and coding standards
scripts/            # Build and utility scripts
```

**Directory Naming**: Use kebab-case for all module directories (e.g., `pop-reference`, `filter-ambiguous`)

## Available Commands

### Testing & Validation
```bash
# Unit tests (mocked LLM calls)
npm run test

# Example tests (real API calls with proper env setup)
npm run examples

# Architecture tests (single run with debug output)
ARCH_LOG=debug npm run arch:once

# Individual architecture test (for performance)
source .env && ARCH_LOG=debug ARCH_SHUFFLE=true vitest --config .vitest.config.arch.js --run index.arch.js
```

### Development
```bash
# Start interactive runner
npm run start

# Development with auto-reload
npm run dev

# Linting
npm run lint
npm run lint:fix

# Run custom scripts
npm run script -- generate-verblet foo
```

### System Notes
- Host system aliases `cat` to `bat` - use `\bat` to access original bat command
- Run paged commands without pager for better output
- Use `source .env` prefix for commands requiring environment variables

## Core Implementation Patterns

### LLM Integration via ChatGPT
```javascript
import chatGPT from '../../lib/chatgpt/index.js';

// Standard call pattern
const result = await chatGPT(prompt, {
  modelOptions: { ...llm },
  ...options
});

// For structured outputs with JSON schema (REQUIRED - never use toObject)
const result = await chatGPT(prompt, {
  modelOptions: {
    modelName: 'fastGoodCheap',
    response_format: {
      type: 'json_schema',
      json_schema: { name: 'result', schema }
    }
  }
});
```

**Key API Behaviors:**
- OpenAI returns JSON **strings** when using `response_format`, not parsed objects
- Root schema must be object type, cannot be array
- The `.items` wrapper is an API requirement for collections

**ChatGPT Module Features:**
- Automatically parses JSON when `response_format` is provided
- `skipResponseParse` option available for edge cases (e.g., primitive values)
- Auto-unwraps simple collection schemas by default (single `items` property containing array)
- Exports `isSimpleCollectionSchema` helper for detecting collection patterns

## JSON Schema Requirements

**CRITICAL: Always use JSON schemas for structured outputs - NEVER use toObject chain**

- **Structured Outputs**: Use `response_format` with `json_schema` for all LLM calls requiring structured data
- **Schema Definition**: Define schemas in `src/json-schemas/` directory
- **Type Safety**: Schemas provide validation and type safety that toObject cannot
- **Consistency**: All verblets and chains must use schemas for parsing LLM responses
- **Testing**: Mock responses should return pre-parsed objects matching the schema structure

```javascript
// CORRECT: Using JSON schema
const schema = {
  type: 'object',
  properties: {
    items: { type: 'array', items: { type: 'string' } }
  }
};

const result = await chatGPT(prompt, {
  modelOptions: {
    modelName: 'fastGoodCheap',
    response_format: {
      type: 'json_schema',
      json_schema: { name: 'result', schema }
    }
  }
});

// INCORRECT: Never use toObject
// const parsed = await toObject(result);
```

### Response Format Guidelines
- Include descriptions in schemas to guide LLM behavior
- Avoid unnecessary nesting (no array of arrays)
- Design formats based on what chains actually need
- Keep structured output close to desired data shape
- Use `responseFormat` naming consistently (not `structuredOutput`)
- Break and repair when making API changes - no legacy support

### Model Selection Strategy

**Privacy First Rule - ABSOLUTE PRIORITY:**
```javascript
// Always use privacy models for sensitive operations
{ modelOptions: { modelName: 'privacy' } }

// Operations requiring privacy models:
// - Personal data anonymization  
// - Sensitive query rephrasing
// - Any PII processing
// - Veiled variants (defaults to privacy)
```

**Model Negotiation Patterns:**
```javascript
// Bulk operations - optimize for speed and cost
{ modelOptions: { negotiate: { fast: true, cheap: true } } }

// Quality-critical operations
{ modelOptions: { negotiate: { good: true } } }

// Complex reasoning tasks
{ modelOptions: { negotiate: { reasoning: true } } }

// Multimodal processing
{ modelOptions: { negotiate: { multi: true } } }

// Default - don't specify modelName, let system use defaults
{ modelOptions: { ...llm } }  // Pass through llm config from options
```

**Model Capability Mapping:**
- `fast`: Quick responses, may sacrifice some quality
- `good`: Balanced performance and quality  
- `cheap`: Cost-optimized selection
- `reasoning`: Advanced logical capabilities
- `multi`: Multimodal (image/text) support
- `privacy`: Local/private model execution (OVERRIDES ALL OTHERS)

### Function Signatures & Natural Language Programming

**Core Philosophy**: Verblets and chains accept **natural language parameters** that can express complex logic and nuanced reasoning impossible with traditional code.

```javascript
// Chains: complex workflows with natural language instructions
export default async function chainName(input, instructions, config = {}) {
  const { chunkSize = 10, maxAttempts = 3, llm, ...options } = config;
  // 'instructions' parameter enables natural language logic
}

// Verblets: transformations with natural language prompts  
export default async function verbletName(input, prompt, options = {}) {
  const { llm, ...restOptions } = options;
  // 'prompt' parameter allows flexible, human-like instructions
}
```

**Examples**: 

See [set-interval chain](src/chains/set-interval/) for examples of adaptive behavior through natural language programming.

### Prompt Construction
```javascript
import { asXML } from '../../prompts/wrap-variable.js';
import { constants as promptConstants } from '../../prompts/index.js';

// XML wrapping for structured data
const listBlock = asXML(items.join('\n'), { tag: 'list' });

// Use prompt constants  
const { onlyJSON, onlyJSONArray, contentIsQuestion } = promptConstants;

// Best practices:
// - Put description/instruction parameters higher in prompts
// - Wrap all caller-supplied content with asXML() for lengthy inputs
// - Use structured XML tags (e.g., <sentence>, <description>, <items>)
```

### Response Processing
```javascript
// Structured outputs (JSON schema responses are pre-parsed)
const parsed = typeof response === 'string' ? JSON.parse(response) : response;
const results = parsed?.items || parsed?.scores || parsed;

// Text outputs
const output = result.trim();
return output ? output.split('\n') : [];
```

### Collection Operations Design
- **Core principle**: Keep `.items` wrapper internal, chains work with arrays
- **Simple collections**: Single `items` property containing array → auto-unwrap
- **Map/Filter/Find**: Return arrays directly to consumers
- **Reduce**: Special case - not a collection operation, builds accumulators

### Reduce Chain Specifics
- Default format: `{accumulator: string}` for simple string accumulation
- Consumers provide custom formats for complex accumulators
- Initial arrays are wrapped transparently when needed
- Don't use `.items` wrapper for reduce - it's not a collection operation

## Testing Standards

### Unit Tests (Vitest)
- **Mock all LLM calls** - Tests should be deterministic and fast
- **Use ordinary assertions** - Standard expect() assertions throughout
- **Avoid mocking fs** - Use real files and directories for simpler, more reliable tests
- Test behavior, not implementation details
- Use realistic mock responses matching actual LLM formats
- Cover clear inputs, ambiguous inputs, edge cases, and validation
- See `guidelines/UNIT_TESTS.md` for complete standards

### Example Tests
- **Use `aiExpect` with fluent interface** - Import as: `import { aiExpect } from '../expect/index.js';`
- **Use pattern**: `await aiExpect(actual).toSatisfy(constraint)`
- All chains and complex verblets need `*.examples.js` files
- Test against real LLM responses for integration validation
- Examples should demonstrate practical usage patterns

### Architecture Tests
- Use `aiArchExpect` for AI-powered code quality analysis
- Use `dependency-cruiser` for fast structural validation
- Test real code against actual guidelines, never mock
- Configure bulk processing for performance optimization
- See `guidelines/ARCHITECTURE_TESTS.md` for complete patterns

## Privacy & Model Selection Guidelines

### Data Classification
1. **Sensitive Data** → Always use `modelName: 'privacy'`
2. **Bulk Processing** → Use `negotiate: { fast: true, cheap: true }`
3. **Quality Critical** → Use `negotiate: { good: true }`
4. **Complex Analysis** → Use `negotiate: { reasoning: true }`
5. **Default Operations** → Don't specify modelName, use system defaults

### Common Model Configurations
```javascript
const MODEL_CONFIGS = {
  privacy: { modelName: 'privacy' },
  bulk: { negotiate: { fast: true, cheap: true } },
  quality: { negotiate: { good: true } },
  reasoning: { negotiate: { reasoning: true } },
  multimodal: { negotiate: { multi: true } }
  // For default: don't specify modelName, use system defaults
};
```

## Module Dependencies

- **Verblets** cannot import **chains** (architectural boundary)
- **Chains** can import any module type  
- **Lib** utilities should minimize dependencies

## Documentation Requirements

See `guidelines/DOCUMENTATION.md` for complete standards:
- README structure: Title, description, cross-references, usage, API, optional features
- Lead with common use cases, realistic examples
- Include model selection rationale for privacy-sensitive operations
- Avoid generic feature lists (bulk processing, retries, etc.)

### Features Section Guidelines
- **Only include Features sections for modules with genuinely powerful or non-obvious capabilities**
- **Avoid listing common features**: Basic bulk processing, standard LLM integration, simple retries
- **Include for exotic features**: Advanced algorithms, sophisticated processing, unique optimizations
- **Keep concise**: 1-2 lines per feature, focus on what makes the module special

### Use Cases Section Guidelines
- **Default**: One compelling example that demonstrates the module's core value proposition
- **Multiple examples only when**: Module has fundamentally different modes of operation or serves multiple distinct user personas
- **Quality over quantity**: One excellent example beats multiple mediocre ones
- **Show unique AI capabilities**: Highlight what makes this module special
- **Realistic scenarios**: Use relatable, practical examples
- **Natural configuration**: Integrate configuration options within examples, not separately

### Anti-patterns to Avoid
- Generic use cases that could apply to any module
- Excessive lists of similar examples
- Contrived scenarios that don't reflect real usage
- Configuration examples without meaningful context
- Bullet point use cases - always show code examples instead of generic bullet lists
- Separate "Examples" sections - integrate examples directly into Use Cases
- Sub-example variations - avoid multiple similar examples within one use case that are variations on the same theme
- **Generic "Advanced Usage" sections** - don't create Advanced Usage sections that only show basic model configuration (llm options, temperature, etc.) unless there's genuinely advanced functionality beyond standard config

### Advanced Usage Guidelines

**Only include "Advanced Usage" sections when showing functionality that goes beyond basic model configuration.**

**Generic model configuration should be documented in core library docs, not individual module READMEs.**

**Advanced Usage sections should only exist for:**
- Complex integration patterns specific to that module
- Specialized configuration options unique to that module  
- Multi-step workflows that demonstrate advanced capabilities
- Error handling patterns specific to that module's domain
- Performance optimization techniques for that specific use case

## Key Implementation Details

- **ES6 Modules**: `import/export` throughout
- **LLM Calls**: Always through `src/lib/chatgpt/index.js`
- **Model Service**: Centralized model negotiation in `src/services/llm-model/`
- **Batch Processing**: Use `chunkSize` config, retry with same model selection
- **Privacy Override**: Privacy models take absolute precedence over all other selections

When implementing new features, examine existing chains/verblets for model selection patterns and follow privacy-first principles. The architecture tests validate adherence to these guidelines.
